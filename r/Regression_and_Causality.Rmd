---
title: "CS112 Assignment 2, Fall 2020"
author: "Evgeniia Barabash"
date: "09/27/2020"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
# Don't change this part of the document
knitr::opts_chunk$set(echo = TRUE)
## install and load the necessary packages
#install.packages("Metrics")
library(lubridate)
library(tree)
library(arm)
library(Matching)
library(boot)
library(randomForest)
library(Metrics)
# we need to set the seed of R's random number generator, in order to produce comparable results 
set.seed(32)
```

## QUESTION 1

#### STEP 1

Create a set of 1000 outcome observations using a data-generating process (DGP) that incorporates a systematic component and a stochastic component (of your choice)

```{r}
# create function to generate normal distributions in the given range
norm <- function(n, mean, sd, a = 20, b = 150){
    qnorm(runif(n, pnorm(a, mean, sd), pnorm(b, mean, sd)), mean, sd)
}
# generation of the data frame
total_area <- norm(200, 50, 25)
rooms <- total_area %/% 15
condition <- sample(c(1,2,3), 200, replace = TRUE)
flat_data <- data.frame(total_area, rooms, condition)
flat_data$price <- with(flat_data, total_area * 854 + rooms * 10000 + condition * 20000 + runif(200, 50000, 150000))
```

#### STEP 2

Tell a 2-3 sentence story about the data generating process you coded up above. What is it about and what each component mean?

This data is about the price of flat based on its total area, number of rooms, and condition (1 is worst, 3 is best). In the price generation I used systematic component with linear relationships with the predictors and the stochastic component to reproduce the variability of the population.


#### STEP 3

Using an incorrect model of the systematic component (of your choice), and using the simulation-based approach covered in class (the `arm` library, etc.), generate 1 prediction for every observation and calculate and report RMSE. Make sure you write out your model and report your RMSE. 

Each prediction should be conditional on the predictor values of the corresponding observation. E.g., when predicting for observation #1, use the predictor values of observation #1.
```{r}
# create an incorrect model with interactions and second-order terms
fit <- lm(price ~ I(total_area^2) + rooms * condition, data = flat_data)

# simulate 1000 runs of linear model and extract means of coefficients
sim.model <- sim(fit, n.sims=1000)
intercept <- mean(coef(sim.model)[,1])
tot.b <- mean(coef(sim.model)[,2])
room.b <- mean(coef(sim.model)[,3])
cond.b <- mean(coef(sim.model)[,4])

predicted.vals <- c()

# predict prices
for (i in 1:200){
  pred <- tot.b*flat_data[i,'total_area'] + room.b*flat_data[i,'rooms'] + cond.b*flat_data[i,'condition'] + intercept
  predicted.vals <- c(predicted.vals, pred)
}

# calculate rmse
rmse(flat_data$price, predicted.vals)
```


#### STEP 4

Using the correct model (correct systematic and stochastic components), and using the simulation-based approach covered in class (the `arm` library, etc.), generate 1 prediction for every observation and calculate & report your RMSE. Once again, write out your model and report your RMSE. 

Each prediction should be conditional on the predictor values of the corresponding observation. E.g., when predicting for observation #1, use the predictor values of observation #1.

```{r}
#This is my primary approach to solve this problem

# create a correct model with true formula
fit <- lm(price ~ ., data = flat_data)

# simulate 1000 runs of linear model and extract means of coefficients
sim.model <- sim(fit, n.sims=1000)
intercept <- mean(coef(sim.model)[,1])
tot.b <- mean(coef(sim.model)[,2])
room.b <- mean(coef(sim.model)[,3])
cond.b <- mean(coef(sim.model)[,4])

predicted.vals <- c()

# predict prices
for (i in 1:200){
  pred <- tot.b*flat_data[i,'total_area'] + room.b*flat_data[i,'rooms'] + cond.b*flat_data[i,'condition'] + intercept
  predicted.vals <- c(predicted.vals, pred)
}
# calculate rmse
rmse(flat_data$price, predicted.vals)
```
```{r}
#This is additional approach without using sim function

pred.list = list()
# sample random 50 obseravations from the data, create a linear model and predict each outcome 1000 times
for (i in 1:1000){
  sample.data <- flat_data[sample(nrow(flat_data), 50, replace = TRUE), ]
  fit <- lm(price ~ ., data = sample.data)
  pred <- predict(fit, flat_data)
  pred.list[[i]] <- pred
}
# find means of predictions
predicted.vals <- c()
for (i in 1:200){
  n <- 0
  for (j in 1:1000){
    n <- n + pred.list[[j]][i] 
  }
  mean <- n/1000
  predicted.vals <- c(predicted.vals, mean)
}
# calculate rmse
rmse(flat_data$price, predicted.vals)
```


#### STEP 5

Which RMSE is larger: The one from the correct model or the one from the incorrect model? Why?

Of course, RMSE is larger for the incorrect model. It is because when we created the dataset, we defined the true relationships between predictors and response; thus, when we use the correct relationships between predictors and response in our lm model we can be the closest to the real responses even though we also have a stochastic component, which represents fundamental uncertainty. On the contrary, incorrect relationships between predictors and response can make the prediction deviate significantly from the correct predictions.


## QUESTION 2

Imagine that you want to create a data viz that illustrates the sensitivity of regression to outlier data points. So, you want to create two figures: 
	
One figure that shows a regression line fit to a 2-dimensional (x and y) scatterplot, such that the regression line clearly has a positive slope. 

```{r}
# generate data
df <- data.frame("x" = runif(20, 0, 10))
df$y <- df$x * runif(20,1,3) +runif(20,0,3)
lm1 <- lm(y~x, data = df)
# print slope of regression line
print(lm1$coef[2])
# plot the scatter plot
plot(df$x, df$y,main="Regression line of x and y",
  xlab="X", ylab="Y")
abline(lm1)
```

And, another figure that shows a regression line with a negative slope fit to a scatter plot of the same data **plus one additional outlier data point**. This one data point is what changes the sign of the regression line’s slope from positive to negative.

```{r}
# generate data
df[nrow(df) + 1,] = c(0,150)
lm2 <- lm(y~x, data = df)
# print slope of regression line
print(lm2$coef[2])
# plot scatter plot
plot(df$x, df$y, main="Regression line of x and y",
  xlab="X", ylab="Y")
abline(lm2)
```

Be sure to label the axes and the title the figures appropriately. Include a brief paragraph that explains the number of observations and how you created the data set and the outlier.

For this question, I generated the dataset of x and y, in which x is the predictor and y is a response. We can see that in the process of generation the data, we defined the relationship between x and y, which is positive (slope = 2.64: if x grows, y grows too). The number of observations is 20. This number is small, so we can make a substantial impact on the slope after adding 1 outlier. Then, I just added the outlier point with x of 0 and y of 150. It could be the mistake in dataset creation because this outlier seems absolutely unrelated to the data.Then, we can see that the slope of the line became negative with the value of -2.16.


## QUESTION 3

#### STEP 1

Using the `laLonde` data set, run a linear regression that models `re78` as a function of `age`, `education`, `re74`, `re75`, `hisp`, and `black`. Note that the `lalonde` data set comes with the package `Matching`.

```{r}
data("lalonde")
lm.lalonde <- lm(re78~age + educ + re74 + re75 + hisp + black, data = lalonde)

```

#### STEP 2

Report coefficients and R-squared. 

```{r}
print('Coefficients')
summary(lm.lalonde)$coef
print('R-Squared')
summary(lm.lalonde)$r.squared


```

Then calculate R-squared by hand and confirm / report that you get the same or nearly the same answer as the summary (`lm`) command. 

Write out hand calculations here.

```{r}
# obtain real and predicted values
predictions <- predict(lm.lalonde, lalonde)
actual <- lalonde$re78
# calculate RSS, TSS and R^2 as the division of them
RSS <- sum((predictions - actual) ^ 2)
TSS <- sum((actual - mean(actual)) ^ 2)
R.Squared <- 1 - RSS/TSS
R.Squared
```

#### STEP 3

Then, setting all the predictors at their means EXCEPT `education`, create a data visualization that shows the 95% confidence interval of the expected values of `re78` as `education` varies from 3 to 16. Be sure to include axes labels and figure titles.

```{r}
#create a vector at which all predictors except for education set to their means

means <- c(colMeans(lalonde))
black <- names(c(table(lalonde$black)))[which.max(c(table(lalonde$black)))]
hisp <- names(c(table(lalonde$hisp)))[which.max(c(table(lalonde$hisp)))]
mar <- names(c(table(lalonde$married)))[which.max(c(table(lalonde$married)))]
deg <- names(c(table(lalonde$nodegr)))[which.max(c(table(lalonde$nodegr)))]
treat <- names(c(table(lalonde$treat)))[which.max(c(table(lalonde$treat)))]
means['black'] = black
means['hisp'] = hisp
means['married'] = mar
means['nodegr'] = deg
means['treat'] = treat
means = as.numeric(means)
names(means) = colnames(lalonde)
means = means[-(9)]
means <- c(1, means)
names(means)[1] = 'intercept'
means
```

```{r}
quan.list <- list()
exp.vector <- c()
# create simulated coefficients for lm
fit <- lm(re78 ~ . , data = lalonde)
sim.fit <- sim(fit, n.sims = 1000)
sim.fit <- coef(sim.fit)
# create a loop that brings random sample of coefficients from the simulated coefficients, finds the mean of them and
# calculate the predictions 1000 times and find the mean of them. Then repeat the process 1000 times to calculate the 
# distribution of expected values
for (educ in seq(from=3,to=16,by=1)){
  exp.vector =c()
  for (j in 1:1000){
    pred.vector = c()
    for (i in 1:1000){
      sample.coefs <- sim.fit[round(runif(min= 1, max = 1000, n = 5)),]
      means.coef <- c(colMeans(sample.coefs))
      means['educ'] = educ
      pred <- sum(means.coef*means)
      pred.vector <- c(pred.vector, pred)
    }
    expected <- mean(pred.vector)
    exp.vector <- c(exp.vector, expected)
  }
  quan1 <- quantile(exp.vector, 0.025)
  quan2 <- quantile(exp.vector, 0.975)
  quan.list[[educ]] = c(quan1,quan2)
}
#plot the 95% confints for different levels of education
plot(x = c(1:10000), y = c(1:10000), type = "n", xlim = c(3,16), main='95% CI for re78 expected value based on education', xlab='education', ylab ='re78')
c2 = rgb(red = 1, green = 0.6, blue = 0, alpha = 1)
for (educ in 3:16) {
  segments(x0=educ, y0=quan.list[[educ]]['2.5%'],
           x1=educ, y1=quan.list[[educ]]['97.5%'], lwd=4, col=c2)}
```


#### STEP 4

Then, do the same thing, but this time for the predicted values of `re78`. Be sure to include axes labels and figure titles.
```{r}
quan.list <- list()
fit <- lm(re78 ~ . , data = lalonde)
sim.fit <- sim(fit, n.sims = 1000)
sim.fit <- coef(sim.fit)
for (educ in seq(from=3,to=16,by=1)){
  pred.vector = c()
  for (i in 1:10000){
    sample.coefs <- sim.fit[round(runif(min= 1, max = 1000, n = 5)),]
    means.coef <- c(colMeans(sample.coefs))
    means['educ'] = educ
    pred <- sum(means.coef*means)
    pred.vector <- c(pred.vector, pred)
  }
  quan1 <- quantile(pred.vector, 0.025)
  quan2 <- quantile(pred.vector, 0.975)
  quan.list[[educ]] = c(quan1,quan2)
}
plot(x = c(1:10000), y = c(1:10000), type = "n", xlim = c(3,16), main='95% CI for re78 predicted value based on education', xlab='education', ylab ='re78')
c2 = rgb(red = 1, green = 0.6, blue = 0, alpha = 1)
for (educ in 3:16) {
  segments(x0=educ, y0=quan.list[[educ]]['2.5%'],
           x1=educ, y1=quan.list[[educ]]['97.5%'], lwd=4, col=c2)}
```


#### STEP 5

Lastly, write a short paragraph with your reflections on this exercise (specifically, the length of intervals for given expected vs. predicted values) and the results you obtained.

The lengths of the intervals for expected values is much smaller than the lengths of the intervals for predicted values. It is because expected values does not contain the fundamental uncertainty (they only account for estimation uncertainty) and thus they have less variance than the predicted values (which account for both fundamental and estimation uncertainty). From the results of the data visualizations, we see that it is possible to predict the results of 9-11 years of education than other years of education. It is because the intervals are the smallest on both expected and predicted values data visualizations. Also, we can see that overally the income in 1978 raise as the years of education raise, which means that more advanced education can increase the salary of a person.


## QUESTION 4

#### STEP 1

Using the `lalonde` data set, run a logistic regression, modeling treatment status as a function of `age`, `education`, `hisp`, `re74` and `re75`. Report and interpret the regression coefficient and 95% confidence intervals for `age` and `education`.
```{r}
log.regr <- glm(treat ~ age + educ + hisp + re74 + re75, data = lalonde, family = 'binomial')
print(coef(log.regr))
print(confint(log.regr))
```

Report and interpret regression coefficient and 95% confidence intervals for `age` and `education` here. 

Regression coefficient for age is about 0.0116, and it means that for every additional year in age the assignment of treatment is 1.16% more likely. The 95% confidence interval for age is from -0.0154 to 0.0385. It means that in most cases (95% of the cases), the possibility of treatment raise from -1.54% to +3.85% per year as age increases.

Regression coefficient for education is about 0.0692, and it means that for every additional year of education the assignment of treatment is 6.92% more likely. The 95% confidence interval for age is from -0.0385 to 0.1795. It means that in most cases (95% of the cases), the possibility of treatment raise from -3.85% to +17.95% per year as education years increases.

#### STEP 2

Use a simple bootstrap to estimate (and report) bootstrapped confidence intervals for `age` and `education` given the logistic regression above. Code the bootstrap algorithm yourself.

```{r}
age.list = c()
edu.list = c()
# for the random samples of 200 observartions we create a logistic regression model and save the needed coefficient
# to the vectors
for (i in 1:1000){
  sample.data <- lalonde[sample(nrow(lalonde), 200, replace = TRUE), ]
  fit <- glm(treat ~ age + educ + hisp + re74 + re75, data = sample.data, family = 'binomial')
  age.list = c(age.list, coef(fit)['age'])
  edu.list = c(edu.list, coef(fit)['educ'])
}
print('age')
quantile(age.list, 0.025)
quantile(age.list, 0.975)
print('education')
quantile(edu.list, 0.025)
quantile(edu.list, 0.975)
```

Report bootstrapped confidence intervals for `age` and `education` here. 

As the result of our bootstrap, we see that for age the confidence interval is from -0.0314 to 0.0553. For the education, it is from -0.095 to 0.272.


#### STEP 3

Then, using the simulation-based approach and the `arm` library, set all the predictors at their means EXCEPT `education`, create a data visualization that shows the 95% confidence interval of the expected values of the probability of receiving treatment as education varies from 3 to 16. Be sure to include axes labels and figure titles.

```{r}
means <- c(colMeans(lalonde))
black <- names(c(table(lalonde$black)))[which.max(c(table(lalonde$black)))]
hisp <- names(c(table(lalonde$hisp)))[which.max(c(table(lalonde$hisp)))]
mar <- names(c(table(lalonde$married)))[which.max(c(table(lalonde$married)))]
deg <- names(c(table(lalonde$nodegr)))[which.max(c(table(lalonde$nodegr)))]
treat <- names(c(table(lalonde$treat)))[which.max(c(table(lalonde$treat)))]
means['black'] = black
means['hisp'] = hisp
means['married'] = mar
means['nodegr'] = deg
means['treat'] = treat
means = as.numeric(means)
names(means) = colnames(lalonde)
means <- c(1, means)
names(means)[1] = 'intercept'
means <- means[c(1,2,3,5,8,9)]
means
```
```{r}
quan.list <- list()
exp.vector <- c()
fit <- glm(treat ~ age + educ + hisp + re74 + re75, data = lalonde, family = 'binomial')
sim.fit <- sim(fit, n.sims = 1000)
sim.fit <- coef(sim.fit)
for (educ in seq(from=3,to=16,by=1)){
  exp.vector =c()
  for (j in 1:500){
    pred.vector = c()
    for (i in 1:500){
      sample.coefs <- sim.fit[round(runif(min= 1, max = 1000, n = 5)),]
      means.coef <- c(colMeans(sample.coefs))
      means['educ'] = educ
      logit <- sum(means.coef*means)
      pred <- (exp(logit) / (1 + exp(logit)))
      pred.vector <- c(pred.vector, pred)
    }
    expected <- mean(pred.vector)
    exp.vector <- c(exp.vector, expected)
  }
  quan1 <- quantile(exp.vector, 0.025)
  quan2 <- quantile(exp.vector, 0.975)
  quan.list[[educ]] = c(quan1,quan2)
}
plot(x = c(0:1), y = c(0:1), type = "n", xlim = c(3,16), main='95% CI for the possibility of treatment EV based on education', xlab='education', ylab ='possibility of treatment')
c2 = rgb(red = 1, green = 0.6, blue = 0, alpha = 1)
for (educ in 3:16) {
  segments(x0=educ, y0=quan.list[[educ]]['2.5%'],
           x1=educ, y1=quan.list[[educ]]['97.5%'], lwd=4, col=c2)}

```

#### STEP 4

Then, do the same thing, but this time for the predicted values of the probability of receiving treatment as education varies from 3 to 16. Be sure to include axes labels and figure titles.

```{r}
fit <- glm(treat ~ age + educ + hisp + re74 + re75, data = lalonde, family = 'binomial')
sim.fit <- sim(fit, n.sims = 1000)
sim.fit <- coef(sim.fit)
quan.list <- list()
for (educ in seq(from=3,to=16,by=1)){
  pred.vector = c()
  for (i in 1:10000){
    sample.coefs <- sim.fit[round(runif(min= 1, max = 1000, n = 5)),]
    means.coef <- c(colMeans(sample.coefs))
    means['educ'] = educ
    logit <- sum(means.coef*means)
    pred <- (exp(logit) / (1 + exp(logit)))
    pred.vector <- c(pred.vector, pred)
  }
  quan1 <- quantile(pred.vector, 0.025)
  quan2 <- quantile(pred.vector, 0.975)
  quan.list[[educ]] = c(quan1,quan2)
}
plot(x = c(0:1), y = c(0:1), type = "n", xlim = c(3,16), main='95% CI for the possibility of treatment PV based on education', xlab='education', ylab ='possibility of treatment')
c2 = rgb(red = 1, green = 0.6, blue = 0, alpha = 1)
for (educ in 3:16) {
  segments(x0=educ, y0=quan.list[[educ]]['2.5%'],
           x1=educ, y1=quan.list[[educ]]['97.5%'], lwd=4, col=c2)}

```

#### STEP 5

Lastly, write a short paragraph with your reflections on this exercise and the results you obtained.

We can see that the picture is similar to the one with re78 predicted and expected values based on education. We can again see that the lengths of the intervals for expected values are much smaller than these of predicted values. It is also because of lack of fundamental uncertainty in the expected values. From this visualizations we can see that even though the percentage of those who receive treatment among differently educated people must be about the same based on the RCT, we see that for the more educated people the possibility of receiving a treatment is higher.


## QUESTION 5


Write the executive summary for a decision brief about the impact of a stress therapy program, targeted at individuals age 18-42, intended to reduce average monthly stress. The program was tested via RCT, and the results are summarized by the figure that you get if you run this code chunk:

```{r}
# Note that if you knit this document, this part of the code won't 
# show up in the final pdf which is OK. We don't need to see the code
# we wrote.

# How effective is a therapy method against stress

# Participants in the study record their stress level for a month.
# Every day, participants assign a value from 1 to 10 for their stress level. 
# At the end of the month, we average the results for each participant.

#adds the confidence interval (first row of the matrix is lower 
# bound, second row is the upper bound)
trt1 = matrix(NA,nrow=2,ncol=7)
ctrl = matrix(NA,nrow=2,ncol=7) 

trt1[,1]=c(3.7, 6.5) #18  
ctrl[,1]=c(5, 8)

trt1[,2]=c(5, 8.5) #22
ctrl[,2]=c(7.5, 9)

trt1[,3]=c(6, 9) #26
ctrl[,3]=c(8.5, 10)

trt1[,4]=c(5, 7) #30
ctrl[,4]=c(6, 8)

trt1[,5]=c(3.5, 5) #34
ctrl[,5]=c(4.5, 7)

trt1[,6]=c(2, 3.5) #38
ctrl[,6]=c(3.5, 6)

trt1[,7]=c(0.5, 2) #42
ctrl[,7]=c(2.5, 5)

# colors to each group
c1 = rgb(red = 0.3, green = 0, blue = 1, alpha = 0.7) #trt1
c2 = rgb(red = 1, green = 0.6, blue = 0, alpha = 1) #trt2
c3 = rgb(red = 0, green = 0.5, blue = 0, alpha = 0.7) #ctrl

# creates the background of the graph
plot(x = c(1:100), y = c(1:100), 
     type = "n", 
     xlim = c(17,43), 
     ylim = c(0,11), 
     cex.lab=1,
     main = "Stress Level - 95% Prediction Intervals", 
     xlab = "Age", 
     ylab = "Average Stress Level per Month", 
     xaxt = "n")

axis(1, at=seq(18,42,by=4), seq(18, 42, by=4))

grid(nx = NA, ny = NULL, col = "lightgray", lty = "dotted",
     lwd=par("lwd"), equilogs = TRUE)

# adds the legend
legend('topright',legend=c('Treatment','Control'),fill=c(c1,c2))

# iterates to add stuff to plot
for (age in seq(from=18,to=42,by=4)) { 
  #treatment
  segments(x0=age-0.2, y0=trt1[1, (age-18)/4+1],
           x1=age-0.2, y1=trt1[2, (age-18)/4+1], lwd=4, col=c1)
  
  #control
  segments(x0=age+0.2, y0=ctrl[1, (age-18)/4+1],
           x1=age+0.2, y1=ctrl[2, (age-18)/4+1], lwd=4, col=c2)
}

```

(Not that it matters, really, but you can imagine that these results were obtained via simulation, just like the results you have hopefully obtained for question 2 above). 

Your executive summary should be between about 4 and 10 sentences long, it should briefly describe the the purpose of the study, the methodology, and the policy implications/prescription. (Feel free to imaginatively but realistically embellish/fill-in-the-blanks with respect to any of the above, since I am not giving you backstory here).

Write your executive summary here.

This summary is about the study that discovered the impact of a stress therapy program, targeted at individuals with ages 18-42, intended to reduce average monthly stress. The study was designed as RCT, in which each unit was randomly assigned to receive a treatment or being in control group and every day the units was asked to provide his/her stress level in the range of 1 to 10. After one month the results was summarized for every unit, and the obtained 95% confidence intervals for the stress level for treatment and control groups were put into the visualization above. As we can see from the results of the study, treatment group on the average reduce stress levels. Mostly, it can be seen for the people after 26 years old as the treatment groups there show substantial decrease of stress level rather than control group. In conclusion, we recommend the stress control therapy used in the experiment to be used with people older than 26 years old.


## QUESTION 6

Can we predict what projects end up being successful on Kickstarter? 

We have data from the [Kickstarter](https://www.kickstarter.com/) company. 

From Wikipedia: Kickstarter is an American public-benefit corporation based in Brooklyn, New York, that maintains a global crowdfunding platform focused on creativity and merchandising. The company's stated mission is to "help bring creative projects to life". As of May 2019, Kickstarter has received more than $4 billion in pledges from 16.3 million backers to fund 445,000 projects, such as films, music, stage shows, comics, journalism, video games, technology, publishing, and food-related projects.

The data is collected by [Mickaël Mouillé](https://www.kaggle.com/kemical) and is last uodated in 2018. Columns are self explanatory. Note that `usd_pledged` is the column `pledged` in US dollars (conversion done by kickstarter) and `usd_pledge_real` is the `pledged` column in real US dollars of the pledged column. Finally, `usd_goal_real` is the column `goal` in real US dollars. You should use the real columns.


So what makes a project successful? Undoubtedly, there are many factors, but perhaps we could set up a prediction problem here, similar to the one from the bonus part of the last assignment where we used GDP to predict personnel contributions. 

We have columns representing the the number of backers, project length, the main category, and the real project goal in USD for each project. 

Let's explore the relationship between those predictors and the dependent variable of interest — the success of a project. 

Instead of running a simple linear regression and calling it a day, let's use cross-validation to make our prediction a little more sophisticated. 

Our general plan is the following: 

1. Build the model on a training data set 
2. Apply the model on a new test data set to make predictions based on the inferred model parameters. 
3. Compute and track the prediction errors to check performance using the mean squared difference between the observed and the predicted outcome values in the test set. 

Let's get to it, step, by step. Make sure you have loaded the necessary packages for this project. 

#### STEP 1: Import & Clean the Data

Import the dataset from this link: https://tinyurl.com/KaggleDataCS112 

Remove any rows that include missing values. 

```{r}
# I cannot read dataframe from the url so I downloaded it and read it from my computer
# df <- read.csv('https://tinyurl.com/KaggleDataCS112')

df <- read.csv('/Users/miss_evgenia/Downloads/ks-projects-201801.csv')
df <- na.omit(df)
```

#### STEP 2: Codify outcome variable

Create a new variable that is either successful or NOT successful and call it `success` and save it in your dataframe. It should take values of 1 (successful) or 0 (unsuccessful).

```{r}
df$success <- ifelse(df$state=='successful', 1, 0)
```

#### STEP 3: Getting the project length variable  

Projects on Kickstarter can last anywhere from 1 - 60 days. Kickstarter claims that projects lasting any longer are rarely successful and campaigns with shorter durations have higher success rates, and create a helpful sense of urgency around your project. Using the package `lubridate` or any other package in R you come across by Googling, create a new column that shows the length of the project by taking the difference between the variable `deadline` and the variable `launched`. Call the new column `length` and save it in your dataframe.

Remove any project length that is higher than 60. 

```{r}
df$deadline <- as.Date(as.character(df$deadline), format="%Y-%m-%d")
df$launched <- as.Date(as.character(df$launched), format="%Y-%m-%d %H:%M:%S")
df$length <- df$deadline - df$launched
df <- df[df$length <= 60,]
```

#### STEP 4: Splitting the data into a training and a testing set

While there are several variations of the k-fold cross-validation method, let’s stick with the simplest one where we just split randomly the dataset into two (k = 2) and split our available data from the link above into a training and a testing (aka validation) set. 

Randomly select 80% of the data to be put in a training set and leave the rest for a test set. 

```{r}
train_ind <- sample(nrow(df), size = round(nrow(df)*0.8))

train <- df[train_ind, ]
test <- df[-train_ind, ]

```


#### STEP 5: Fitting a model 

Use a logistic regression to find what factors determine the chances a project is successful. Use the variable indicating whether a project is successful or not as the dependent variables (Y) and number of backers, project length, main category of the project, and the real project goal as independent variables. Make sure to use the main category as factor.

```{r}
kick.log <- glm(success ~ backers + main_category + length + usd_goal_real, data = train, family = 'binomial')
```


#### STEP 6: Predictions

Use the model you’ve inferred from the previous step to predict the success outcomes in the test set.

```{r}
preds.test <- predict(kick.log, test, type = 'response')
preds.test <- ifelse(preds.test > 0.5, 1, 0)
```

#### STEP 7: How well did it do? 

Report the Root Mean Squared Error (RMSE) of the predictions for the training and the test sets. 

```{r}
preds.train <- predict(kick.log, train, type = 'response')
preds.train <- ifelse(preds.train > 0.5, 1, 0)
print('Train:')
print('Confusion matrix:')
table(preds.train,train$success)
print('Misclassification rate')
mean(preds.train != train$success)
print('')
print('Test:')
print('Confusion matrix:')
table(preds.test,test$success)
print('Misclassification rate')
mean(preds.test != test$success)
```

#### Step 8: LOOCV method

Apply the leave-one-out cross validation (LOOCV) method to the training set. What is the RMSE of the training and test sets. How similar are the RMSEs?

```{r}
loocv_ind_tr <- sample(nrow(df), size = round(nrow(df)*0.01))
loocv_tr <- df[loocv_ind_tr, ]

loocv_ind_te <- sample(nrow(df), size = round(nrow(df)*0.01))
loocv_te <- df[loocv_ind_te, ]

kick.log.cv <- glm(success ~ backers + main_category + length + usd_goal_real, data = loocv_tr, family = 'binomial')
cv <- cv.glm(loocv_tr, kick.log.cv)
```

```{r}
print('Train Misclassification Rate:')
cv$delta[1]
print('Test Misclassification Rate:')
preds.test <- predict(kick.log.cv, loocv_te, type = 'response')
preds.test <- ifelse(preds.test > 0.5, 1, 0)
mean(preds.test != loocv_te$success)
```


#### Step 9: Explanations

Compare the RMSE from the simple method to the LOOCV method?

How do data scientists really use cross-validation? How is the approach in this project differ from real-world cases? Give an example to make your point!

We can see that the misclassification rate of the test sample is better done by the simple method as the train test misclassification rate is 0.104, while the real test misclassification rate is 0.102, so the difference is minimal. However, for the LOOCV approach (and I think mainly because we used a little sample) the train misclassification rate is 0.0856, while the test misclassification rate is 0.1069.

I think scientists mostly use k-fold CV because it can give the opportunity to not overfit, while do less computations than LOOCV and create less variance as the models are mostly not highly correlated as the input samples are not nearly the same for every model as in LOOCV. Also, I think scientists will bring all the data to do cross-validation, even though it can take days of calculations as the accuracy is the highest priority.

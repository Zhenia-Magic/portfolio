{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melanoma detection with Keras and Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, our primary goal is to develop a reliable and accurate convolutional neural network (CNN) to predict if the image of a mole contains the melanoma features for early detection of skin cancer. This model may help health specialists detect the first signals of emerging disease and effectively deal with it. \n",
    "\n",
    "To achieve the goal we set, we will use a variety of machine learning techniques that can help us develop the most accurate model we can do with the existing data, technical resources, and time constraints.\n",
    "\n",
    "Firstly, before we go to the actual code, we will discuss the main features of the method we will use to train our model and the model itself.\n",
    "\n",
    "## Model:\n",
    "\n",
    "### Efficient Net B7\n",
    "\n",
    "Image classification is a widely studied topic that already has a lot of advanced models that can obtain a high score (90+ %) on their own without adding any extra layers. As we will use the already existing model structure for our assignment, we will use **transfer learning** (a machine learning technique where a model trained on one task is re-purposed on a second related task (Brownlee, 2019))\n",
    "\n",
    "Transfer learning is an excellent opportunity to obtain great results without reinventing the wheel and adding extra complexity to our neural network. While it is possible to create a model with the same accuracy, it will take much time unnecessarily because we already have good models to use.\n",
    "\n",
    "EfficientNet is a group of state-of-an-art convolutional neural networks that was firstly introduced by Mingxing Tan and Quoc V. Le in their work *EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks* (2019). Even though it can provide excellent accuracy on image datasets (CIFAR-100 (91.7%), Flowers (98.8%), etc.) EfficientNet is light-weight, small, and fast compared to other popular models for image classification. For example, the simplest EfficientNet model B0 contains only 5,330,564 parameters, while ResNet-50 (one of the popular image classification models) has 23,534,592 settings and at the same time underperforms the former model (Zhang, 2019).\n",
    "For example, as we can see in the graph below made by Tan and Le (2019), EfficientNets achieves the best accuracy within the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/tensorflow/tpu/master/models/official/efficientnet/g3doc/params.png)\n",
    "<center> <i>Figure 1.</i> <b>Model Size vs. ImageNet Accuracy.</b> All numbers are for single-crop, single-model. EfficientNets significantly outperform other ConvNets. In particular, EfficientNet-B7 achieves new state-of-the-art 84.4% top-1 accuracy but being 8.4x smaller and 6.1x faster than GPipe. EfficientNet-B1 is 7.6x smaller and 5.7x faster than ResNet-152 (Tan & Le, 2019).</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The main building block for EfficientNet is **MBConv**, an inverted bottleneck conv, originally known as MobileNetV2. By using shortcuts directly between the bottlenecks, which connects a much fewer number of channels compared to expansion layers, combined with depthwise separable convolution, which effectively reduces computation by almost a factor of k<sup>2</sup>, compared to traditional layers. Where k stands for the kernel size, specifying the height and width of the 2D convolution window\" (Zhang, 2019). \n",
    "\n",
    "EfficientNet B7 architecture is complex yet being developed by using combination of simple blocks shown on the scheme below (Agarwal, 2020).\n",
    "\n",
    "![](https://miro.medium.com/max/2000/1*cwMpOJNhwOeosjwW-usYvA.png)\n",
    "<center> <i>Figure 2.</i> <b>Bulding Blocks of EfficientNets.</b></center>\n",
    "\n",
    "\n",
    "These 5 types of blocks are combined into sub-blocks (Agarwal, 2020).\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*snN5M6WXlqHVFAwi17H9Mw.png)\n",
    "<center> <i>Figure 3.</i> <b>Bulding Sub-blocks of EfficientNets.</b></center>\n",
    "\n",
    "\n",
    "The overall architecture contains 813 layers, and they are organised from sub-blocks the way it shown on the figure below (Agarwal, 2020).\n",
    "\n",
    "![](https://miro.medium.com/max/2000/1*9LkWH_LUPi5QD1k-QcUA2g.png)\n",
    "<center> <i>Figure 4.</i> <b>EfficientNet B7 Architecture.</b></center>\n",
    "\n",
    "\n",
    "One of the best features implemented in EfficientNets is compound scaling, which makes the process of training fast and efficient and makes it easy to use this model with high-resolution images due to the balance between different single-dimension scalings (width, resolution, depth). \n",
    "\n",
    "As we could see, EfficientNet B7 is a great choice for our image classification assignment because of its efficiency in terms of technical resources, time, and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques:\n",
    "\n",
    "In this section, we look at the techniques implemented to increase the overall accuracy and generalization of CNN. Of course, we will further explore how exactly we will use them in the code later in this paper.\n",
    "\n",
    "\n",
    "### 1. K-fold cross-validation\n",
    "\n",
    "K-fold cross-validation is the popular technique to ensure that our model can perform well on the unseen test data. It is widely used to prevent the model from overfitting (failure to generalize to the other set of points (test data) because of a too tight fit to the training data). Therefore, we need to use data points from our dataset as validation data (data we will use to calculate the accuracy of our model). \n",
    "\n",
    "Even though there are several ways to perform this split, they have some severe drawbacks. For example, train_test_split from the scikit-learn library divides the dataset into two random parts (one to train the model, and one to test its accuracy), thus, lacking the part of the information that is stored into test records (because they are not used for training). As we have a very imbalanced dataset, because of this split we can put all melanomas into test dataset (as the standard split 80/20, will put about 6600 images into test data, while there are only 500 melanoma images at all in the dataset (if we use original data)), so our neural network will not see the malicious images and will not be able to train from only benign ones. \n",
    "\n",
    "K-fold cross-validation is the tool that can help in this situation. It is based on split of the dataset into K (number of folds) folds and subsequent series of trainings on K-1 of them and testings on 1. Overally, this process can be visualized this way:\n",
    "![](https://www.researchgate.net/profile/B_Aksasse/publication/326866871/figure/fig2/AS:669601385947145@1536656819574/K-fold-cross-validation-In-addition-we-outline-an-overview-of-the-different-metrics-used.jpg)\n",
    "<center> <i>Figure 5.</i> <b>K-fold Cross-Validation Process.</b> (Reference 6)</center>\n",
    "\n",
    "As we use all the records for both training and testing, we do not lack any data, and the accuracy of the model does not depend on the random train-test split. K-fold cross-validation makes the model less biased, so it will make it easier to generalize.\n",
    "\n",
    "### 2. Extended Dataset\n",
    "\n",
    "If we look at the original dataset (\"SIIM-ISIC Melanoma Classification\", 2020), it is imbalanced as the ratio of malicious images to benign is about 0.018, which means that the number of no disease pictures is tremendously higher than the number of melanoma ones.\n",
    "\n",
    "This imbalance can bias our results a lot as even if the model predicts all images to be benign, its accuracy will be about 98%. This case will be a terrible thing because this model will be useless. Therefore, we will use an extended dataset (Reference 8) with more malicious and benign images, so our model will be able to train from more images and detect more patterns. The extended dataset contains about seventy thousand images versus thirty thousand in the original one, and the ratio of malicious images to benign is increased.\n",
    "\n",
    "### 3. Data Augmentations\n",
    "\n",
    "Data augmentations are an essential tool to increase the diversity of the data without actually collecting new data. It can help the model generalize better as CNN will use diverse data to learn patterns from, and it will lead to the general increase of the possible patterns the model can extract. We use different simple data augmentations in the code: rotations, flips, changes in hues, contrast, brightness, and saturation. These necessary enhancements can diversify our data, yet keep it understandable and meaningful.\n",
    "\n",
    "### 4. TTA (Test-Time Augmentation)\n",
    "\n",
    "It is another technique that can provide better accuracy in the testing data. Test-Time Augmentation means that we do several epochs of test data label prediction with different augmentations used for the test images and then bring the average of these predictions to the final submission file. Using TTA, we eliminate bias that can be brought by a single usage of the model as we use model multiple times on the same image with different augmentations, which also helps as they can make it easier for the neural network to recognize the pattern.\n",
    "\n",
    "### 5. Automatic Mixed Precision\n",
    "\n",
    "Automatic Mixed Precision is used to speed up the neural network training by changing the FP32 type to FP16 (32-bit floating-point to 16-bit). The numbers changed to 16-bit requires twice less memory, so the RAM usage is lowered. Also, the operations are much faster with FP16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we go to the actual code. Firstly, we need to import all packages and libraries and install *efficient net* (transfer learning model) and enable *mixed-precision*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed packages and libraries\n",
    "!pip install -q efficientnet\n",
    "!export TF_ENABLE_AUTO_MIXED_PRECISION=1\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "import efficientnet.tfkeras as efn\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use TPU to train our neural network as it is quite difficult (in terms of memory storage) and long for CPU to perform these operations. Therefore, we need to prepare our kernel for the TPU usage with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Detect TPU and configure the system appropriately\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we detect if the TPU is available and connect with the TPU gRPC server running on the TPU VM using **TPUClusterResolver()** and print the TPU configuration using **tpu.master**. Then, if the TPU is presented, we configure the variable used in the following code to enable TPU to train the model. Moreover, we create a variable in which the number of replicas is stored. It will be necessary for subsequent calculations.\n",
    "\n",
    "If TPU is not presented, we set the variable that will either use GPU or CPU to do the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to load the dataset using **KaggleDatasets** package. We need to obtain the GCS path of the dataset as it is impossible to use TPU any other way with the Kaggle data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dataset from Kaggle to use it with TPU\n",
    "DATASET = '512x512-melanoma-tfrecords-70k-images'\n",
    "GCS_PATH = KaggleDatasets().get_gcs_path(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as we have the data downloaded, we need to set some required parameters:\n",
    "\n",
    "- SEED - a different seed produces a different K-fold split.\n",
    "\n",
    "\n",
    "- SIZE - is a Python list of image sizes.\n",
    "\n",
    "\n",
    "- BATCH_SIZES - the number of training examples utilized in one iteration (32 is a default batch size, then we multiply it by the number of replicas in TPU).\n",
    "\n",
    "\n",
    "- EPOCHS - the number of passes of the entire training dataset the machine learning algorithm has completed.\n",
    "\n",
    "\n",
    "- TTA - test-time augmentation. Each test image is randomly augmented and predicted TTA times and the average prediction is used.\n",
    "\n",
    "\n",
    "- LR - learning rate. A tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.\n",
    "\n",
    "\n",
    "- WARMUP - a parameter for learning rate schedule that will set the epoch on which the learning rate will start to decrease.\n",
    "\n",
    "\n",
    "- LABEL_SMOOTHING - a regularization technique for classification problems to prevent the model from predicting the labels too confidently during training and generalizing poorly.\n",
    "\n",
    "\n",
    "- AUTO - parameter that prompts the tf.data runtime to tune the value dynamically at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set required parameters\n",
    "SEED = 42\n",
    "SIZE = [512,512]\n",
    "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
    "EPOCHS = 10\n",
    "TTA = 4\n",
    "LR = 0.00004\n",
    "WARMUP = 5\n",
    "LABEL_SMOOTHING = 0.05\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing we must mention is the seed. We already set a parameter for seed, but now we need to make a function **seed_everything** that assures us that the model training can be reproduced. After definition of this function, we call it to actually set seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix random seed for reproducibility\n",
    "def seed_everything(SEED):\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to set one more function - for image augmentation. As we already discussed, augmentation is an important feature to increase the diversity of the dataset by applying several modifications to the original images. \n",
    "\n",
    "Here, in the function **data_augment**, we use rotations (90, 180, 270, or 360 degrees); left-right and up-down flips; random hue adjustment by a small delta (0.01); random saturation, contrast and brightness adjustments with the slight difference from the original image.\n",
    "\n",
    "These augmentations cannot produce any loss of relevant information such as when pixel drops or gaussian-blur, so they can be used to increase the accuracy by improving the generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to augment the image with different augmentations such as rotations,\n",
    "#flips, changes in hues, saturation, contrast, and brightness to enhance the generalization of the model\n",
    "def data_augment(image, label=None, seed=SEED):\n",
    "    image = tf.image.rot90(image,k=np.random.randint(4))\n",
    "    image = tf.image.random_flip_left_right(image, seed=seed)\n",
    "    image = tf.image.random_flip_up_down(image, seed=seed)\n",
    "    image = tf.image.random_hue(image, 0.01)\n",
    "    image = tf.image.random_saturation(image, 0.7, 1.3)\n",
    "    image = tf.image.random_con(Deotte, 2020)ΩdWQtrast(image, 0.8, 1.2)\n",
    "    image = tf.image.random_brightness(image, 0.1)\n",
    "    if label is None:\n",
    "        return image\n",
    "    else:\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data format we use in the following program is TFRecords, a simple format for storing a sequence of binary records. We already have the dataset of images with metadata presented in this format (Deotte, 2020), so we need to encode the TFRecords from the files to work with the data.\n",
    "\n",
    "To decode images and labels (as well as image names for test data) from TFRecords, we need to identify the features we need to extract and parse each record (called example) TFRecordDataset.\n",
    "\n",
    "The format of the records in the data we use is stated in the description of the dataset (Deotte, 2020):\n",
    "\n",
    "```python\n",
    "feature = {\n",
    "  'image': _bytes_feature,\n",
    "  'image_name': _bytes_feature,\n",
    "  'patient_id': _int64_feature,\n",
    "  'sex': _int64_feature,\n",
    "  'age_approx': _int64_feature,\n",
    "  'anatom_site_general_challenge': _int64_feature,\n",
    "  'source': _int64_feature,\n",
    "  'target': _int64_feature\n",
    "}```\n",
    "\n",
    "From all these features, we need only the **image** (provided in jpeg), **image_name** (for test data), and **target** (for train and validation data). \n",
    "\n",
    "There can be the question: why don't we use other metadata? Moreover, there is an answer to this fair question: metadata gives us worse results! It can be unclear why it happens. However, most possibly, the reason is the inequality of 'weights' that has the image and the metadata. For example, the image itself can be the most critical feature in determining if the person has melanoma, while with the usage of other metadata (without the needed weight adjustment), we diminish the impact of the image with possibly unrelated details. \n",
    "\n",
    "We have the code that can be used to prove that the metadata can make the model's accuracy less.\n",
    "\n",
    "```python\n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        # tf.string means bytestring\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), \n",
    "        # shape [] means single element\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        # meta features\n",
    "        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"sex\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64)\n",
    "        \n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['target'], tf.float32)\n",
    "    # meta features\n",
    "    data = {}\n",
    "    data['age_approx'] = tf.cast(example['age_approx'], tf.int32)\n",
    "    data['sex'] = tf.cast(example['sex'], tf.int32)\n",
    "    data['anatom_site_general_challenge'] = tf.cast(tf.one_hot(example['anatom_site_general_challenge'], 7), tf.int32)\n",
    "    # returns a dataset of (image, label, data)\n",
    "    return image, label, data\n",
    "    \n",
    "    # this function parse our image and also get our image_name (id) to perform predictions\n",
    "def read_unlabeled_tfrecord(example):\n",
    "    UNLABELED_TFREC_FORMAT = {\n",
    "        # tf.string means bytestring\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), \n",
    "        # shape [] means single element\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
    "        # meta features\n",
    "        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"sex\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    image_name = example['image_name']\n",
    "    # meta features\n",
    "    data = {}\n",
    "    data['age_approx'] = tf.cast(example['age_approx'], tf.int32)\n",
    "    data['sex'] = tf.cast(example['sex'], tf.int32)\n",
    "    data['anatom_site_general_challenge'] = tf.cast(tf.one_hot(example['anatom_site_general_challenge'], 7), tf.int32)\n",
    "    # returns a dataset of (image, key, data)\n",
    "    return image, image_name, data\n",
    "```\n",
    "\n",
    "As we could see, we need to encode only images with labels (train) and image names (test). We do this with the help of documentation presented on the official website of Tensorflow (Reference 9). \n",
    "\n",
    "Thus, we create functions first to decode the image and reshape it to the right size (**decode_image** function) through the usage of decode_jpeg method as our images are stored in the jpeg format inside the TFRecords, cast method to present the image as an array of floats between 0 and 1 (normalize the numbers in arrays through dividing by 255), and reshape method to change the size of the image if needed.\n",
    "\n",
    "Second, we create two functions to read image and label (for training data) or image and image name (for test data) from TFRecords (**read_labeled_trfrecord** and **read_unlabeled_tfrecord**). Here, we define the needed format by using labels (image, target, image_name) and their values, decode images with our function **decode_image**), and cast the value if needed (i.e., the target is cast to tf.int32 format).\n",
    "\n",
    "The last function in the following piece of code is **load_dataset**, which is used to load the TFRecords Dataset from the filenames of the files we need to open. Here, we also identify if the dataset we load is labeled (train, validation) or not (test) to use the appropriate function to load the records (**read_labeled_trfrecord** or **read_unlabeled_tfrecord**) and if the dataset needs to be ordered (test, validation) or not (train) to keep the order or shuffle the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function  to decode image from jpeg and reshape it to the right size\n",
    "def decode_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3) \n",
    "    image = tf.cast(image, tf.float32)/255.0\n",
    "    image = tf.reshape(image, [*SIZE, 3])\n",
    "    return image\n",
    "\n",
    "# create a function to read the image and target (train/valid data) from tfrecord    \n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64),  }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['target'], tf.int32)\n",
    "    return image, label \n",
    "\n",
    "# create a function to read the image and image name (test data) from tfrecord\n",
    "def read_unlabeled_tfrecord(example):\n",
    "    UNLABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string), }\n",
    "    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    image_name = example['image_name']\n",
    "    return image, image_name\n",
    "\n",
    "# create a function to read full dataset from tfrecords\n",
    "def load_dataset(filenames, labeled=True, ordered=False):\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False\n",
    "    dataset = (tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) \n",
    "              .with_options(ignore_order)\n",
    "              .map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create functions that will use our previous basic functions to get three required datasets (train, test, and validation). Their names are **get_training dataset**, **get_test_dataset**, and **get_validation_dataset**. They read the filenames and parameters (labeled and ordered). \n",
    "\n",
    "At this moment, we must understand why certain types of datasets having specific parameter values:\n",
    "\n",
    "- ### train data: \n",
    "        \n",
    "     - **labeled = True**: \n",
    "     In train data, we have targets (labels) in TFRecords, so when we read these files, we will need to read them using read_labeled_tfrecord function.\n",
    "     \n",
    "     - **Ordered = False**: \n",
    "     In train data, we need to shuffle the data, and if it is not ordered, it will eliminate the bias of the model being trained on data ordered in the biased way (i.e., containing only melanoma images in the beginning).\n",
    "\n",
    "            \n",
    "- ### validation data: \n",
    "        \n",
    "     - **labeled = True**: \n",
    "     In validation data, we have targets (labels) in TFRecords, so when we read these files, we will need to read them using read_labeled_tfrecord function.\n",
    "     \n",
    "     - **Ordered = True**: \n",
    "     As we are using validation dataset as a test dataset to obtain accuracy while training the model, we need to keep it ordered to measure the accuracy in the same way for all epochs (as lack of order can provide different results on accuracy not based on the model itself but the order of records).\n",
    "\n",
    "- ### test data: \n",
    "        \n",
    "     - **labeled = False**: \n",
    "     In test data, we do not have targets (labels) in TFRecords, so when we read these files, we will need to read them using read_unlabeled_tfrecord function.\n",
    "     \n",
    "     - **Ordered = True**: \n",
    "     In test data, we need to keep the order as the CSV we will fill is ordered in this way and also because lack of order can provide different results on accuracy not based on the model itself but the order of records.\n",
    "\n",
    "\n",
    "Other methods we use are:\n",
    "\n",
    "- **repeat**: to repeat the training dataset for multiple epochs.\n",
    "\n",
    "- **shuffle**: to shuffle the training dataset even more (as we already ignored the order).\n",
    "\n",
    "- **batch**: to divide the dataset into small pieces, each size of which is the batch size.\n",
    "\n",
    "- **prefetch**: Prefetching can reduce the time needed to train and test the model using a specific order of reading and using files. 'Prefetching overlaps the preprocessing and model execution of a training step. While the model executes training step s, the input pipeline is reading the data for step s+1. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data.'(https://www.tensorflow.org/guide/data_performance).\n",
    "\n",
    "Moreover, we use our **data_augment** function to augment images in train (increase diversity of the input data) and test (use of TTA) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create function to load training dataset, augment images in it, shuffle, and batch it        \n",
    "def get_training_dataset(filenames, labeled = True, ordered = False):\n",
    "    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n",
    "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
    "    # the training dataset must repeat for several epochs\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "#create function to load validation dataset, augment images in it, shuffle, and batch it\n",
    "def get_validation_dataset(filenames, labeled = True, ordered = True):\n",
    "    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    dataset = dataset.prefetch(AUTO) \n",
    "    return dataset\n",
    "\n",
    "#create function to load test dataset and batch it\n",
    "def get_test_dataset(filenames, labeled = False, ordered = True):\n",
    "    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n",
    "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    dataset = dataset.prefetch(AUTO) \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we go to the actual analysis. Firstly we return a list of files that match the pattern '/train.tfrec' and '/test.tfrec' with the usage of **tf.io.gfile.glob**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read train and test filenames from the data directory\n",
    "train_filenames = tf.io.gfile.glob(GCS_PATH + '/train*.tfrec')\n",
    "test_filenames = tf.io.gfile.glob(GCS_PATH + '/test*.tfrec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the report should be informative, we include the function to display a random image with seven variations of augmentations. As a result of this function, we see how the augmentations we use to change the images. To display the images, we use **matplotlib.pyplot**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display random picture with its augmentations\n",
    "def plot_transform(num_images):\n",
    "    plt.figure(figsize=(30,10))\n",
    "    x = load_dataset(train_filenames, labeled=False)\n",
    "    image,_ = iter(x).next()\n",
    "    for i in range(1,num_images+1):\n",
    "        plt.subplot(1,num_images+1,i)\n",
    "        plt.axis('off')\n",
    "        image = data_augment(image=image)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "plot_transform(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we loaded and prepared the data, we can define our model and its features. We bring EfficientNetB7 with the defined size of an image, pre-trained weights (as for imagenet), average pooling (to not lose any pixels when do pooling while bringing the average of every 4 pixels of the image), and excluded top level (dense, as we define it ourselves later with sigmoid activation). After loading EfficientNetB7, we define the last layer of the network as a dense layer with one output and sigmoid activation, which means we will have the number between 0 and 1 that will show the probability of being class 1 rather than class 0 (It is usually used for binary classification, while softmax is used for multiclass one).\n",
    "\n",
    "Then, we define the optimizer (**Adam** is a classic, modern and accurate optimizer), loss function (**Binary Cross Entropy** is a good loss practice when dealing with binary classifier), and metrics (**accuracy** that shows the frequency with which *y_pred* matches *y_true*, **AUC** that computes the approximate AUC (Area under the curve) via a Riemann sum).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to define a model that will be trained\n",
    "#here, I use transfer learning with the Efficient net B7 model\n",
    "#which is one of the most advanced model for image classification nowadays\n",
    "\n",
    "def get_model():\n",
    "    with strategy.scope():\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            efn.EfficientNetB7(input_shape=(*SIZE, 3),weights='imagenet',pooling='avg',include_top=False),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "    \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = LABEL_SMOOTHING),\n",
    "            metrics=['accuracy',tf.keras.metrics.AUC(name='auc')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more important component of the high accuracy is a correctly defined learning rate. Here, we use **cosine schedule with warmup**. The warmup is a central epoch (5 in this paper, as we have a total of 10 epochs), before which the learning rate is growing and after which it is decreasing. 'Compared to some widely used strategies including exponential decay and step decay, the cosine decay decreases the learning rate slowly at the beginning, and then\n",
    "becomes almost linear decreasing in the middle, and slows down again at the end. It potentially improves the training progress'(Zhang, 2018).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that defines the learning rate schedule\n",
    "#using cosine schedule with warmups\n",
    "def get_cosine_schedule_with_warmup(lr,num_warmup_steps, num_training_steps, num_cycles=0.5):\n",
    "    def lrfn(epoch):\n",
    "        if epoch < num_warmup_steps:\n",
    "            return (float(epoch) / float(max(1, num_warmup_steps))) * lr\n",
    "        progress = float(epoch - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr\n",
    "\n",
    "    return tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n",
    "\n",
    "lr_schedule= get_cosine_schedule_with_warmup(lr=LR,num_warmup_steps=WARMUP,num_training_steps=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the correct functioning of our model we need the number of all types of images as well as the steps per epoch (the metric that means how many steps need to be done to train all the batches of images in one epoch), we define the function **count_data_items**  that finds the sum of the files that follow a specific format in the filenames list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to count data items in the filenames directory\n",
    "def count_data_items(filenames):\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "#count the number of train, valid, and test files as well as steps per epochs\n",
    "NUM_TRAINING_IMAGES = int(count_data_items(train_filenames) * 0.8)\n",
    "NUM_VALIDATION_IMAGES = int(count_data_items(train_filenames) * 0.2)\n",
    "NUM_TEST_IMAGES = count_data_items(test_filenames)\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary step in this assignment is to write the function (called **train**) that will train our model. Firstly, we create an empty list in which the five trained models will be stored (5 because we have k-folds) and split the train files into five folds with **KFold** function. Then we create a loop that will create five models with the usage of different combinations of 4 training and one validation folds. \n",
    "\n",
    "The important thing here is the session clearance for each model: it is used to empty the memory from previous weights before starting to count new ones. This way, we use memory more efficiently and avoid the memory overload problem.\n",
    "\n",
    "Another feature we implement is the early stopping, which is used to stop training if the validation AUC is not improved during 5 (patience) epochs in a row at least by 0.0001 (min_delta). Even though this callback is unlikely to be used when we have just ten epochs, it can be a good practice if we want to increase the number of epochs to 20 or more.\n",
    "\n",
    "After training five models with slightly different accuracies, we must combine them to get the most from our k-folds cross-validation. We do this with the addition of TTA. Hence, we predict the target for every row in the submission data frame 4 times with five models (as TTA is 4). To elaborate on it, we bring the forth-part of the average of predictions of 5 models for each record four times and then summarize them in order to get the average target from our TTA and k-folds.\n",
    "\n",
    "When we have the finalized targets, we generate a CSV file with them as well as image names (submission df) and return submission data frame as the result of our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the function that will perform training with the k-folds cross entropy\n",
    "def train(folds = 5):\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    # seed everything\n",
    "    seed_everything(SEED)\n",
    "\n",
    "    kfold = KFold(folds, shuffle = True, random_state = SEED)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train_filenames)):\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        train_dataset = get_training_dataset([train_filenames[x] for x in trn_ind], labeled = True, ordered = False)\n",
    "        val_dataset = get_validation_dataset([train_filenames[x] for x in val_ind], labeled = True, ordered = True)\n",
    "        K.clear_session()\n",
    "        model = get_model()\n",
    "        # using early stopping using val loss\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_auc', mode = 'max', patience = 5, \n",
    "                                                      verbose = 1, min_delta = 0.0001, restore_best_weights = True)\n",
    "        history = model.fit(train_dataset, \n",
    "                            steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                            epochs = EPOCHS,\n",
    "                            callbacks = [early_stopping, lr_schedule],\n",
    "                            validation_data = val_dataset,\n",
    "                            verbose = 1)\n",
    "        models.append(model)\n",
    "    \n",
    "    print('\\n')\n",
    "    print('-'*50)\n",
    "    submission_df = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')\n",
    "    print('Computing predictions...')\n",
    "    #using TTA to predict the test dataset predictions\n",
    "    for i in range(TTA):\n",
    "        test_ds = get_test_dataset(test_filenames, labeled = False, ordered = True)\n",
    "        test_images_ds = test_ds.map(lambda image, image_name: image)\n",
    "        probabilities = np.average([np.concatenate(models[i].predict(test_images_ds)) for i in range(folds)], axis = 0)\n",
    "        test_ids_ds = test_ds.map(lambda image, image_name: image_name).unbatch()\n",
    "        # all in one batch\n",
    "        test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')\n",
    "        pred_df = pd.DataFrame({'image_name': test_ids, 'target': probabilities})\n",
    "        temp = submission_df.copy()   \n",
    "        del temp['target']  \n",
    "        submission_df['target'] += temp.merge(pred_df,on=\"image_name\")['target']/TTA\n",
    "    print('Generating submission.csv file...')\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to train the model with our predefined function **train**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After predicting the test data labels, it is essential to look at how much melanoma images were detected. It will convince us that our model is not useless (as we mentioned before when we thought about 'fake' accuracy based on predictions that all images are benign)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.round(submission_df['target'].values)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "1. Brownlee, J. (2019, September 16). A Gentle Introduction to Transfer Learning for Deep Learning. Retrieved July 20, 2020, from https://machinelearningmastery.com/transfer-learning-for-deep-learning/\n",
    "\n",
    "\n",
    "2. SIIM-ISIC Melanoma Classification. (2020). Retrieved July 20, 2020, from https://www.kaggle.com/c/siim-isic-melanoma-classification/data\n",
    "\n",
    "\n",
    "3. Tan, M., & Le, Q.V. (2019). EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. ArXiv, abs/1905.11946.\n",
    "\n",
    "\n",
    "4. Zhang, C. (2019). How to do Transfer learning with Efficientnet. Retrieved July 22, 2020, from https://www.dlology.com/blog/transfer-learning-with-efficientnet\n",
    "\n",
    "\n",
    "5. Agarwal, V. (2020, May 31). Complete Architectural Details of all EfficientNet Models. Retrieved July 22, 2020, from https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142\n",
    "\n",
    "\n",
    "6. Classification algorithms in Data Mining - Scientific Figure on ResearchGate. Available from: https://www.researchgate.net/figure/K-fold-cross-validation-In-addition-we-outline-an-overview-of-the-different-metrics-used_fig2_326866871 [accessed 23 Jul, 2020]\n",
    "\n",
    "\n",
    "7. 3.1. Cross-validation: Evaluating estimator performance¶. (n.d.). Retrieved July 23, 2020, from https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    " \n",
    "8. Deotte, C. (2020, June 06). 512x512 Melanoma TFRecords 70k Images. Retrieved July 23, 2020, from https://www.kaggle.com/cdeotte/512x512-melanoma-tfrecords-70k-images\n",
    "\n",
    "\n",
    "9. TFRecord and tf.Example &nbsp;: &nbsp; TensorFlow Core. (n.d.). Retrieved July 28, 2020, from https://www.tensorflow.org/tutorials/load_data/tfrecord?hl=en\n",
    "\n",
    "\n",
    "10.  Zhang, C. (2018). Bag of Tricks for Image Classification with Convolutional Neural Networks in Keras. Retrieved August 02, 2020, from https://www.dlology.com/blog/bag-of-tricks-for-image-classification-with-convolutional-neural-networks-in-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
